{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6461e274",
   "metadata": {},
   "source": [
    "# Project CogniVerse: The Interactive Lab\n",
    "\n",
    "Welcome to the CogniVerse Interactive Lab! The purpose of this notebook is to deconstruct the complex, multimodal RAG pipeline used in our main `cogniverse_app.py` script.\n",
    "\n",
    "Here, we will go step-by-step through the entire process, from raw data to the final, synthesized answer. By running each cell, you will be able to see the exact output and data structures at every stage. This is the best way to build a strong intuition for how this advanced RAG architecture works.\n",
    "\n",
    "**Our Goal:** To understand the **Multi-Vector Retriever** architecture in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110e429",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "First, we'll import all the necessary libraries and set up our configuration. We will use the same LLMs as our main application.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdbe8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core LangChain and Utility Imports ---\n",
    "import uuid\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define which local Ollama LLMs to use for different tasks.\n",
    "TEXT_SUMMARY_MODEL = \"phi3:mini\"\n",
    "IMAGE_SUMMARY_MODEL = \"llava\"\n",
    "FINAL_RESPONSE_MODEL = \"llava\"\n",
    "\n",
    "print(\"✅ Setup Complete. Libraries imported and models configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518d776",
   "metadata": {},
   "source": [
    "## 2. Our Sample Data\n",
    "Instead of processing the entire 600-page PDF, we'll use a small, representative sample of data. This sample includes\n",
    "1.  A text chunk (a paragraph).\n",
    "2.  A table (represented as HTML, which is what `unstructured` provides).\n",
    "3.  An image (represented as a base64 string, as if we had loaded it from a file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa548de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sample Text Chunk ---\n",
    "sample_text = \"Virtual clusters are built with VMs installed at distributed servers from one or more physical clusters. The VMs in a virtual cluster are interconnected logically by a virtual network. This allows for dynamic properties such as nodes being either physical or virtual machines, and the size of the cluster can grow or shrink dynamically. The failure of physical nodes may disable some VMs, but the failure of VMs will not pull down the host system.\"\n",
    "\n",
    "# --- Sample Table (as HTML) ---\n",
    "sample_table_html = \"\"\"<table>\n",
    "  <tr>\n",
    "    <th>Cloud Model</th>\n",
    "    <th>Ownership</th>\n",
    "    <th>Best For</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Public Cloud</td>\n",
    "    <td>Provider</td>\n",
    "    <td>Standardization, Flexibility</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Private Cloud</td>\n",
    "    <td>Client/Organization</td>\n",
    "    <td>Customization, Security</td>\n",
    "  </tr>\n",
    "</table>\"\"\"\n",
    "\n",
    "# --- Sample Image (as a placeholder base64 string) ---\n",
    "# This is a real base64 string for a tiny 1x1 red pixel PNG image. \n",
    "# In our real app, this would be a large string from a real diagram.\n",
    "sample_image_b64 = \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/wcAAwAB/epv2AAAAABJRU5ErkJggg==\"\n",
    "\n",
    "print(\"--- Sample Data Loaded ---\")\n",
    "print(\"Text:\", sample_text[:50] + \"...\")\n",
    "print(\"Table:\", sample_table_html[:50].replace('\\n', ' ') + \"...\")\n",
    "print(\"Image:\", sample_image_b64[:50] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375d7e1",
   "metadata": {},
   "source": [
    "## 3. The Summarization Step\n",
    "Now, we'll perform the first key step of the Multi-Vector Retriever architecture: creating a short, concise summary for each piece of our raw data. These summaries will be used for the similarity search.\n",
    "\n",
    "**Technical Note:** This step can be slow because it involves calling our local LLMs. We're doing it here interactively to see the output. In our main app, this is the slow, one-time process that runs when the vector store is first built.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de1a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize our LLMs ---\n",
    "text_llm = OllamaLLM(model=TEXT_SUMMARY_MODEL, temperature=0)\n",
    "image_llm = OllamaLLM(model=IMAGE_SUMMARY_MODEL, temperature=0)\n",
    "\n",
    "# --- Define the summarization prompts ---\n",
    "text_summary_prompt = ChatPromptTemplate.from_template(\"Provide a very concise, one-sentence summary of the following text from a computer science textbook: {element}\")\n",
    "table_summary_prompt = ChatPromptTemplate.from_template(\"Provide a very concise, one-sentence summary of the following table from a computer science textbook: {element}\")\n",
    "\n",
    "# --- Create the summarization chains ---\n",
    "text_summarizer = text_summary_prompt | text_llm | StrOutputParser()\n",
    "table_summarizer = table_summary_prompt | text_llm | StrOutputParser()\n",
    "\n",
    "# --- Generate the summaries ---\n",
    "print(\"Generating summaries (this may take a moment)...\\n\")\n",
    "\n",
    "text_summary = text_summarizer.invoke({\"element\": sample_text})\n",
    "table_summary = table_summarizer.invoke({\"element\": sample_table_html})\n",
    "\n",
    "# For the image, we call the multimodal LLM directly\n",
    "image_summary_msg = image_llm.invoke([\n",
    "    HumanMessage(content=[\n",
    "        {\"type\": \"text\", \"text\": \"Summarize this image in one sentence for a search index:\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{sample_image_b64}\"}\n",
    "    ])\n",
    "])\n",
    "image_summary = image_summary_msg.content\n",
    "\n",
    "print(\"--- Generated Summaries ---\")\n",
    "print(f\"[Text Summary]: {text_summary}\")\n",
    "print(f\"[Table Summary]: {table_summary}\")\n",
    "print(f\"[Image Summary]: {image_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79873e",
   "metadata": {},
   "source": [
    "## 4. Building the Multi-Vector Retriever\n",
    "\n",
    "This is the most complex and important part of the architecture. We will build the retriever, which consists of two main storage components:\n",
    "\n",
    "1.  **The Vector Store (`ChromaDB`):** This will store the vector embeddings of our **summaries**.\n",
    "2.  **The Document Store (`InMemoryStore`):** This will store our **original, full-sized data** (the long text, the HTML table, and the image's base64 string).\n",
    "    \n",
    "    The retriever's job is to link these two stores together.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18751207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize the Retriever's Components ---\n",
    "# We'll use an in-memory version of Chroma for this lab to keep it simple.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"cogniverse_lab_summaries\",\n",
    "    embedding_function=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n",
    "# This is a simple in-memory dictionary to hold our original data.\n",
    "docstore = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create the main retriever object, connecting the two stores.\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# --- Populate the Retriever --- \n",
    "\n",
    "# 1. Generate unique IDs for each of our original documents.\n",
    "text_id = str(uuid.uuid4())\n",
    "table_id = str(uuid.uuid4())\n",
    "image_id = str(uuid.uuid4())\n",
    "\n",
    "# 2. Store the ORIGINAL documents in the docstore, linking them with their IDs.\n",
    "retriever.docstore.mset([\n",
    "    (text_id, Document(page_content=sample_text, metadata={\"source_type\": \"text\"})),\n",
    "    (table_id, Document(page_content=sample_table_html, metadata={\"source_type\": \"table\"})),\n",
    "    (image_id, Document(page_content=sample_image_b64, metadata={\"source_type\": \"image\"})),\n",
    "])\n",
    "\n",
    "# 3. Store the SUMMARY documents in the vectorstore. \n",
    "#    Crucially, each summary's metadata contains the ID of its original document.\n",
    "retriever.vectorstore.add_documents([\n",
    "    Document(page_content=text_summary, metadata={id_key: text_id}),\n",
    "    Document(page_content=table_summary, metadata={id_key: table_id}),\n",
    "    Document(page_content=image_summary, metadata={id_key: image_id}),\n",
    "])\n",
    "\n",
    "print(\"✅ Multi-Vector Retriever successfully built and populated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1c9cc",
   "metadata": {},
   "source": [
    "## 5. Retrieval in Action: Seeing the Magic\n",
    "Now, let's test our retriever. We will ask a question that is clearly related to our sample table.\n",
    "\n",
    "Watch the two-step process:\n",
    "\n",
    "The retriever will first perform a similarity search on the summaries in the vector store.\n",
    "\n",
    "It will then use the ID from the best-matching summary to retrieve the original, full-sized document from the docstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb962c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the differences between public and private clouds?\"\n",
    "\n",
    "print(f\"Searching for: '{query}'...\\n\")\n",
    "\n",
    "# --- Step 1 (Internal): Similarity search on summaries ---\n",
    "# This is what the retriever does behind the scenes. We'll simulate it here.\n",
    "retrieved_summaries = retriever.vectorstore.similarity_search(query, k=1)\n",
    "best_summary = retrieved_summaries[0]\n",
    "\n",
    "print(\"--- Step 1: Best Matching Summary Found ---\")\n",
    "print(\"Content:\", best_summary.page_content)\n",
    "print(\"Linked Original Doc ID:\", best_summary.metadata[id_key])\n",
    "\n",
    "# --- Step 2 (Automatic): Retrieving the original documents ---\n",
    "# This is the main call to the retriever. It handles everything automatically.\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"\\n--- Step 2: Full Original Document Retrieved from Docstore ---\")\n",
    "print(\"Number of docs retrieved:\", len(retrieved_docs))\n",
    "print(\"Type of content:\", retrieved_docs[0].metadata['source_type'])\n",
    "print(\"Full Content:\\n\", retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d364c697",
   "metadata": {},
   "source": [
    "Success! As you can see, the query about clouds correctly matched the summary of our table. Then, the retriever used the linked ID to fetch the full, original HTML table from the docstore. This is the core principle of the Multi-Vector Retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2503fa",
   "metadata": {},
   "source": [
    "## 6. The Final Prompt and Generation\n",
    "The final step is to take the retrieved documents (which can be a mix of text, tables, and images) and format them into a single prompt for our powerful multimodal LLM, llava."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca48fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_final_prompt(docs):\n",
    "    \"\"\"Prepares the context for the multimodal LLM, separating text and images.\"\"\"\n",
    "    prompt_content = []\n",
    "    prompt_content.append({\"type\": \"text\", \"text\": \"You are an expert study buddy... (Full prompt text)\"})\n",
    "\n",
    "    for doc in docs:\n",
    "        if doc.metadata.get('source_type') == 'image':\n",
    "            prompt_content.append({\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{doc.page_content}\"})\n",
    "        else:\n",
    "            prompt_content.append({\"type\": \"text\", \"text\": f\"\\n[Text/Table Context]:\\n{doc.page_content}\"})\n",
    "\n",
    "    prompt_content.append({\"type\": \"text\", \"text\": \"\\n--- CONTEXT END ---\\n\"})\n",
    "    return prompt_content\n",
    "\n",
    "# --- Let's simulate the full process for a new query ---\n",
    "final_query = \"Explain virtual clusters and show me a diagram.\"\n",
    "\n",
    "# 1. Retrieve the relevant docs (this time it should get both the text and the image)\n",
    "final_retrieved_docs = retriever.get_relevant_documents(final_query, k=2)\n",
    "\n",
    "# 2. Format them for the LLM\n",
    "final_prompt_content = format_for_final_prompt(final_retrieved_docs)\n",
    "final_prompt_content.append({\"type\": \"text\", \"text\": f\"\\nQuestion: {final_query}\"})\n",
    "\n",
    "print(\"--- Final Prompt Sent to LLaVA ---\")\n",
    "import json\n",
    "print(json.dumps(final_prompt_content, indent=2))\n",
    "\n",
    "# 3. (Simulated) Call the LLM and get the answer\n",
    "print(\"\\n--- (Simulated) Final Answer from LLaVA ---\")\n",
    "print(\"\"\"Based on the textbook, a **virtual cluster** is a collection of Virtual Machines (VMs) that are interconnected by a logical, virtual network. They are highly flexible because their size can grow or shrink dynamically as needed.\\n\\nThe provided diagram, which appears to be a simple placeholder, illustrates the concept that visual information can be included alongside text to explain complex topics.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97645109",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This interactive lab has demonstrated the complete, end-to-end workflow of an advanced, multimodal RAG system.\n",
    "\n",
    "We have seen how to:\n",
    "\n",
    "Take raw, mixed-media content.\n",
    "\n",
    "Generate concise summaries for each piece of content.\n",
    "\n",
    "Build a MultiVectorRetriever that links these summaries to their original, full-sized documents.\n",
    "\n",
    "Perform a search that accurately retrieves a mix of text, tables, and images.\n",
    "\n",
    "Construct a final, rich prompt to be sent to a multimodal LLM.\n",
    "\n",
    "This exact logic is what powers our full cogniverse_app.py script. By understanding these fundamental steps, you now have a deep intuition for how the entire application works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogniverse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
