{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6461e274",
   "metadata": {},
   "source": [
    "# Project CogniVerse: The Interactive Lab\n",
    "\n",
    "Welcome to the CogniVerse Interactive Lab! The purpose of this notebook is to deconstruct the complex, multimodal RAG pipeline used in our main `cogniverse_app.py` script.\n",
    "\n",
    "Here, we will go step-by-step through the entire process, from raw data to the final, synthesized answer. By running each cell, you will be able to see the exact output and data structures at every stage. This is the best way to build a strong intuition for how this advanced RAG architecture works.\n",
    "\n",
    "**Our Goal:** To understand the **Multi-Vector Retriever** architecture in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110e429",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "First, we'll import all the necessary libraries and set up our configuration. We will use the same LLMs as our main application.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acdbe8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup Complete. Libraries imported and models configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Core LangChain and Utility Imports ---\n",
    "import uuid\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define which local Ollama LLMs to use for different tasks.\n",
    "TEXT_SUMMARY_MODEL = \"phi3:mini\"\n",
    "IMAGE_SUMMARY_MODEL = \"llava\"\n",
    "FINAL_RESPONSE_MODEL = \"llava\"\n",
    "\n",
    "print(\"✅ Setup Complete. Libraries imported and models configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518d776",
   "metadata": {},
   "source": [
    "## 2. Our Sample Data\n",
    "Instead of processing the entire 600-page PDF, we'll use a small, representative sample of data. This sample includes\n",
    "1.  A text chunk (a paragraph).\n",
    "2.  A table (represented as HTML, which is what `unstructured` provides).\n",
    "3.  An image (represented as a base64 string, as if we had loaded it from a file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa548de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Data Loaded ---\n",
      "Text: Virtual clusters are built with VMs installed at d...\n",
      "Table: <table>   <tr>     <th>Cloud Model</th>     <th>Ow...\n",
      "Image: iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0...\n"
     ]
    }
   ],
   "source": [
    "# --- Sample Text Chunk ---\n",
    "sample_text = \"Virtual clusters are built with VMs installed at distributed servers from one or more physical clusters. The VMs in a virtual cluster are interconnected logically by a virtual network. This allows for dynamic properties such as nodes being either physical or virtual machines, and the size of the cluster can grow or shrink dynamically. The failure of physical nodes may disable some VMs, but the failure of VMs will not pull down the host system.\"\n",
    "\n",
    "# --- Sample Table (as HTML) ---\n",
    "sample_table_html = \"\"\"<table>\n",
    "  <tr>\n",
    "    <th>Cloud Model</th>\n",
    "    <th>Ownership</th>\n",
    "    <th>Best For</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Public Cloud</td>\n",
    "    <td>Provider</td>\n",
    "    <td>Standardization, Flexibility</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Private Cloud</td>\n",
    "    <td>Client/Organization</td>\n",
    "    <td>Customization, Security</td>\n",
    "  </tr>\n",
    "</table>\"\"\"\n",
    "\n",
    "# --- Sample Image (as a placeholder base64 string) ---\n",
    "# This is a real base64 string for a tiny 1x1 red pixel PNG image. \n",
    "# In our real app, this would be a large string from a real diagram.\n",
    "sample_image_b64 = \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/wcAAwAB/epv2AAAAABJRU5ErkJggg==\"\n",
    "\n",
    "print(\"--- Sample Data Loaded ---\")\n",
    "print(\"Text:\", sample_text[:50] + \"...\")\n",
    "print(\"Table:\", sample_table_html[:50].replace('\\n', ' ') + \"...\")\n",
    "print(\"Image:\", sample_image_b64[:50] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375d7e1",
   "metadata": {},
   "source": [
    "## 3. The Summarization Step\n",
    "Now, we'll perform the first key step of the Multi-Vector Retriever architecture: creating a short, concise summary for each piece of our raw data. These summaries will be used for the similarity search.\n",
    "\n",
    "**Technical Note:** This step can be slow because it involves calling our local LLMs. We're doing it here interactively to see the output. In our main app, this is the slow, one-time process that runs when the vector store is first built.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de1a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries (this may take a moment)...\n",
      "\n",
      "--- Generated Summaries ---\n",
      "[Text Summary]: A computer science textbook describes a flexible network where both hardware-based servers and software-simulated machines are interconnected through virtual networks in distributed clusters that can dynamically adjust size without affecting overall stability or pulling down systems upon individual failures.\n",
      "[Table Summary]: A table comparing Public and Private Cloud models in terms of ownership and ideal use cases: public clouds are provider-owned for standardized flexibility while private clouds are client/organization owned for customization and security.\n",
      "[Image Summary]:  The image shows a person standing on a beach, looking out at the ocean. \n"
     ]
    }
   ],
   "source": [
    "# --- Initialize our LLMs ---\n",
    "text_llm = OllamaLLM(model=TEXT_SUMMARY_MODEL, temperature=0)\n",
    "image_llm = OllamaLLM(model=IMAGE_SUMMARY_MODEL, temperature=0)\n",
    "\n",
    "# --- Define the summarization prompts ---\n",
    "text_summary_prompt = ChatPromptTemplate.from_template(\"Provide a very concise, one-sentence summary of the following text from a computer science textbook: {element}\")\n",
    "table_summary_prompt = ChatPromptTemplate.from_template(\"Provide a very concise, one-sentence summary of the following table from a computer science textbook: {element}\")\n",
    "\n",
    "# --- Create the summarization chains ---\n",
    "text_summarizer = text_summary_prompt | text_llm | StrOutputParser()\n",
    "table_summarizer = table_summary_prompt | text_llm | StrOutputParser()\n",
    "\n",
    "# --- Generate the summaries ---\n",
    "print(\"Generating summaries (this may take a moment)...\\n\")\n",
    "\n",
    "text_summary = text_summarizer.invoke({\"element\": sample_text})\n",
    "table_summary = table_summarizer.invoke({\"element\": sample_table_html})\n",
    "\n",
    "# For the image, we call the multimodal LLM directly\n",
    "image_summary_msg = image_llm.invoke([\n",
    "    HumanMessage(content=[\n",
    "        {\"type\": \"text\", \"text\": \"Summarize this image in one sentence for a search index:\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{sample_image_b64}\"}\n",
    "    ])\n",
    "])\n",
    "image_summary = image_summary_msg\n",
    "\n",
    "print(\"--- Generated Summaries ---\")\n",
    "print(f\"[Text Summary]: {text_summary}\")\n",
    "print(f\"[Table Summary]: {table_summary}\")\n",
    "print(f\"[Image Summary]: {image_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79873e",
   "metadata": {},
   "source": [
    "## 4. Building the Multi-Vector Retriever\n",
    "\n",
    "This is the most complex and important part of the architecture. We will build the retriever, which consists of two main storage components:\n",
    "\n",
    "1.  **The Vector Store (`ChromaDB`):** This will store the vector embeddings of our **summaries**.\n",
    "2.  **The Document Store (`InMemoryStore`):** This will store our **original, full-sized data** (the long text, the HTML table, and the image's base64 string).\n",
    "    \n",
    "    The retriever's job is to link these two stores together.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41a81c",
   "metadata": {},
   "source": [
    "### The Core Concept: The Library and the Card Catalog Analogy\n",
    "Imagine your goal is to find information in a massive library. You have two options:\n",
    "\n",
    "The Naive Way: Go to the first shelf, pull out every book, read the whole thing, put it back, and repeat for thousands of books. This is slow and inefficient.\n",
    "\n",
    "The Smart Way (The Multi-Vector Retriever): Go to the card catalog. Each card is a tiny summary of a book (title, author, a short description). You can quickly scan these small summaries (this is our fast vector search). When you find the perfect summary card, you don't read the card for your answer. You look at the call number on the card (e.g., 796.357). This number is our doc_id. It's a unique pointer that tells you exactly where the full, original book is on the shelf. You then use that ID to go to the shelf and get the actual book.\n",
    "\n",
    "Our code implements this exact \"smart\" system. It separates the searchable \"summaries\" from the \"original content.\"\n",
    "\n",
    "`The Card Catalog (vectorstore)`: This is our Chroma database. It only stores the small, searchable summaries.\n",
    "\n",
    "`The Bookshelves (docstore)`: This is our InMemoryStore. It stores the large, original data (full text, tables, images).\n",
    "\n",
    "`The Linking Mechanism (doc_id)`: The unique ID in each summary's metadata is the \"call number\" that links the card catalog to the bookshelf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18751207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\anaconda3\\envs\\cogniverse_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_27036\\673196191.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Vector Retriever successfully built and populated!\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize the Retriever's Components ---\n",
    "# We'll use an in-memory version of Chroma for this lab to keep it simple.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"cogniverse_lab_summaries\",\n",
    "    embedding_function=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n",
    "# This is a simple in-memory dictionary to hold our original data.\n",
    "docstore = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create the main retriever object, connecting the two stores.\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# --- Populate the Retriever --- \n",
    "\n",
    "# 1. Generate unique IDs for each of our original documents.\n",
    "# The uuid.uuid4() function generates a random, universally unique identifier. Think of this as creating a unique library barcode for each of our original items.\n",
    "text_id = str(uuid.uuid4())\n",
    "table_id = str(uuid.uuid4())\n",
    "image_id = str(uuid.uuid4())\n",
    "\n",
    "# 2. Store the ORIGINAL documents in the docstore, linking them with their IDs.\n",
    "retriever.docstore.mset([\n",
    "    (text_id, Document(page_content=sample_text, metadata={\"source_type\": \"text\"})),\n",
    "    (table_id, Document(page_content=sample_table_html, metadata={\"source_type\": \"table\"})),\n",
    "    (image_id, Document(page_content=sample_image_b64, metadata={\"source_type\": \"image\"})),\n",
    "])\n",
    "\n",
    "# 3. Store the SUMMARY documents in the vectorstore. \n",
    "#    Crucially, each summary's metadata contains the ID of its original document.\n",
    "retriever.vectorstore.add_documents([\n",
    "    Document(page_content=text_summary, metadata={id_key: text_id}),\n",
    "    Document(page_content=table_summary, metadata={id_key: table_id}),\n",
    "    Document(page_content=image_summary, metadata={id_key: image_id}),\n",
    "])\n",
    "\n",
    "print(\"✅ Multi-Vector Retriever successfully built and populated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670e19f",
   "metadata": {},
   "source": [
    "The .mset() function (multi-set) takes a list of (key, value) pairs and stores them in our InMemoryStore.\n",
    "\n",
    "Technical Intricacy: The docstore is now a simple key-value dictionary. The key is our unique ID, and the value is the full, original Document object containing the raw content.\n",
    "\n",
    "Visualizing the docstore (Our Bookshelves):\n",
    "| Key (ID) | Value (The Full, Original Content) |\n",
    "| :--- | :--- |\n",
    "| \"doc-uuid-123\" | Document(page_content=\"Virtual clusters are built with VMs...\") |\n",
    "| \"doc-uuid-456\" | Document(page_content=\"<table><tr><th>Cloud Model...</table>\") |\n",
    "| \"doc-uuid-789\" | Document(page_content=\"iVBORw0KGgoAAA...\") |\n",
    "\n",
    "\n",
    "Visualizing the vectorstore (Our Card Catalog):\n",
    "| Content (The Summary) | Vector (The Embedding) | Metadata (The Link!) |\n",
    "| :--- | :--- | :--- |\n",
    "| \"A text describing virtual clusters...\" | [0.1, -0.5, 0.3, ...] | {'doc_id': 'doc-uuid-123'} |\n",
    "| \"A table comparing Public and Private...\"| [-0.2, 0.8, 0.9, ...] | {'doc_id': 'doc-uuid-456'} |\n",
    "| \"A simple red pixel image...\" | [0.7, 0.1, -0.4, ...] | {'doc_id': 'doc-uuid-789'} |\n",
    "\n",
    "The MultiVectorRetriever is now fully built. It knows about both storage locations and the doc_id key that links them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1c9cc",
   "metadata": {},
   "source": [
    "## 5. Retrieval in Action: Seeing the Magic\n",
    "Now, let's test our retriever. We will ask a question that is clearly related to our sample table.\n",
    "\n",
    "Watch the two-step process:\n",
    "\n",
    "The retriever will first perform a similarity search on the summaries in the vector store.\n",
    "\n",
    "It will then use the ID from the best-matching summary to retrieve the original, full-sized document from the docstore.\n",
    "\n",
    "The Automated Two-Step Process 🤖\n",
    "When you call retriever.get_relevant_documents(query), here’s what happens behind the scenes:\n",
    "\n",
    "`Search the Summaries:` The retriever first takes your query and performs a similarity search on the small, efficient summaries located in the vectorstore. This is the fastest way to find a match.\n",
    "\n",
    "`Fetch the Original:` Once it finds the best-matching summary (or summaries), it looks at that summary's metadata to find the linked ID (the doc_id). It then uses this ID as a key to instantly retrieve the full, original document from the docstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb962c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'What are the differences between public and private clouds?'...\n",
      "\n",
      "--- Step 1: Best Matching Summary Found ---\n",
      "Content: A table comparing Public and Private Cloud models in terms of ownership and ideal use cases: public clouds are provider-owned for standardized flexibility while private clouds are client/organization owned for customization and security.\n",
      "Linked Original Doc ID: 86686935-f662-46ee-a5e8-9a3f53dbbd85\n",
      "\n",
      "--- Step 2: Full Original Document Retrieved from Docstore ---\n",
      "Number of docs retrieved: 3\n",
      "Type of content: table\n",
      "Full Content:\n",
      " <table>\n",
      "  <tr>\n",
      "    <th>Cloud Model</th>\n",
      "    <th>Ownership</th>\n",
      "    <th>Best For</th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Public Cloud</td>\n",
      "    <td>Provider</td>\n",
      "    <td>Standardization, Flexibility</td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Private Cloud</td>\n",
      "    <td>Client/Organization</td>\n",
      "    <td>Customization, Security</td>\n",
      "  </tr>\n",
      "</table>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_27036\\1567847842.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the differences between public and private clouds?\"\n",
    "\n",
    "print(f\"Searching for: '{query}'...\\n\")\n",
    "\n",
    "# --- Step 1 (Internal): Similarity search on summaries ---\n",
    "# This is what the retriever does behind the scenes. We'll simulate it here.\n",
    "retrieved_summaries = retriever.vectorstore.similarity_search(query, k=1)\n",
    "best_summary = retrieved_summaries[0]\n",
    "\n",
    "print(\"--- Step 1: Best Matching Summary Found ---\")\n",
    "print(\"Content:\", best_summary.page_content)\n",
    "print(\"Linked Original Doc ID:\", best_summary.metadata[id_key])\n",
    "\n",
    "# --- Step 2 (Automatic): Retrieving the original documents ---\n",
    "# This is the main call to the retriever. It handles everything automatically.\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"\\n--- Step 2: Full Original Document Retrieved from Docstore ---\")\n",
    "print(\"Number of docs retrieved:\", len(retrieved_docs))\n",
    "print(\"Type of content:\", retrieved_docs[0].metadata['source_type'])\n",
    "print(\"Full Content:\\n\", retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d364c697",
   "metadata": {},
   "source": [
    "Success! As you can see, the query about clouds correctly matched the summary of our table. Then, the retriever used the linked ID to fetch the full, original HTML table from the docstore. This is the core principle of the Multi-Vector Retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2503fa",
   "metadata": {},
   "source": [
    "## 6. The Final Prompt and Generation\n",
    "The final step is to take the retrieved documents (which can be a mix of text, tables, and images) and format them into a single prompt for our powerful multimodal LLM, llava."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca48fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Prompt Sent to LLaVA ---\n",
      "[\n",
      "  {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"You are an expert study buddy... (Full prompt text)\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"\\n[Text/Table Context]:\\nVirtual clusters are built with VMs installed at distributed servers from one or more physical clusters. The VMs in a virtual cluster are interconnected logically by a virtual network. This allows for dynamic properties such as nodes being either physical or virtual machines, and the size of the cluster can grow or shrink dynamically. The failure of physical nodes may disable some VMs, but the failure of VMs will not pull down the host system.\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"\\n[Text/Table Context]:\\n<table>\\n  <tr>\\n    <th>Cloud Model</th>\\n    <th>Ownership</th>\\n    <th>Best For</th>\\n  </tr>\\n  <tr>\\n    <td>Public Cloud</td>\\n    <td>Provider</td>\\n    <td>Standardization, Flexibility</td>\\n  </tr>\\n  <tr>\\n    <td>Private Cloud</td>\\n    <td>Client/Organization</td>\\n    <td>Customization, Security</td>\\n  </tr>\\n</table>\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"image_url\",\n",
      "    \"image_url\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/wcAAwAB/epv2AAAAABJRU5ErkJggg==\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"\\n--- CONTEXT END ---\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"\\nQuestion: Explain virtual clusters and show me a diagram.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "--- (Simulated) Final Answer from LLaVA ---\n",
      "Based on the textbook, a **virtual cluster** is a collection of Virtual Machines (VMs) that are interconnected by a logical, virtual network. They are highly flexible because their size can grow or shrink dynamically as needed.\n",
      "\n",
      "The provided diagram, which appears to be a simple placeholder, illustrates the concept that visual information can be included alongside text to explain complex topics.\n"
     ]
    }
   ],
   "source": [
    "def format_for_final_prompt(docs):\n",
    "    \"\"\"Prepares the context for the multimodal LLM, separating text and images.\"\"\"\n",
    "    prompt_content = []\n",
    "    prompt_content.append({\"type\": \"text\", \"text\": \"You are an expert study buddy... (Full prompt text)\"})\n",
    "\n",
    "    for doc in docs:\n",
    "        if doc.metadata.get('source_type') == 'image':\n",
    "            prompt_content.append({\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{doc.page_content}\"})\n",
    "        else:\n",
    "            prompt_content.append({\"type\": \"text\", \"text\": f\"\\n[Text/Table Context]:\\n{doc.page_content}\"})\n",
    "\n",
    "    prompt_content.append({\"type\": \"text\", \"text\": \"\\n--- CONTEXT END ---\\n\"})\n",
    "    return prompt_content\n",
    "\n",
    "# --- Let's simulate the full process for a new query ---\n",
    "final_query = \"Explain virtual clusters and show me a diagram.\"\n",
    "\n",
    "# 1. Retrieve the relevant docs (this time it should get both the text and the image)\n",
    "final_retrieved_docs = retriever.get_relevant_documents(final_query, k=2)\n",
    "\n",
    "# 2. Format them for the LLM\n",
    "final_prompt_content = format_for_final_prompt(final_retrieved_docs)\n",
    "final_prompt_content.append({\"type\": \"text\", \"text\": f\"\\nQuestion: {final_query}\"})\n",
    "\n",
    "print(\"--- Final Prompt Sent to LLaVA ---\")\n",
    "import json\n",
    "print(json.dumps(final_prompt_content, indent=2))\n",
    "\n",
    "# 3. (Simulated) Call the LLM and get the answer\n",
    "print(\"\\n--- (Simulated) Final Answer from LLaVA ---\")\n",
    "print(\"\"\"Based on the textbook, a **virtual cluster** is a collection of Virtual Machines (VMs) that are interconnected by a logical, virtual network. They are highly flexible because their size can grow or shrink dynamically as needed.\\n\\nThe provided diagram, which appears to be a simple placeholder, illustrates the concept that visual information can be included alongside text to explain complex topics.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97645109",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This interactive lab has demonstrated the complete, end-to-end workflow of an advanced, multimodal RAG system.\n",
    "\n",
    "We have seen how to:\n",
    "\n",
    "Take raw, mixed-media content.\n",
    "\n",
    "Generate concise summaries for each piece of content.\n",
    "\n",
    "Build a MultiVectorRetriever that links these summaries to their original, full-sized documents.\n",
    "\n",
    "Perform a search that accurately retrieves a mix of text, tables, and images.\n",
    "\n",
    "Construct a final, rich prompt to be sent to a multimodal LLM.\n",
    "\n",
    "This exact logic is what powers our full cogniverse_app.py script. By understanding these fundamental steps, you now have a deep intuition for how the entire application works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogniverse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
